{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 558 Final Project: Code Part 1\n",
    "**Including and building off of Homework 4 and 5 suggested exercises**  \n",
    "Alyssa Goodrich   \n",
    "June 4, 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load necessary packages\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from IPython.core.debugger import Tracer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://s3.amazonaws.com/data558ag/Project/features.csv', sep=',', header=None)\n",
    "labels = pd.read_csv('https://s3.amazonaws.com/data558ag/Project/labels.csv', sep=',', header=None)\n",
    "testFeatures = pd.read_csv('https://s3.amazonaws.com/data558ag/Project/testFeatures.csv', sep=',', header=None)\n",
    "testLabels = pd.read_csv('https://s3.amazonaws.com/data558ag/Project/testLabels.csv', sep=',', header=None, dtype = int)\n",
    "testLabels = np.array(testLabels)[:,0]\n",
    "dataLabels = pd.read_csv('https://s3.amazonaws.com/data558ag/Project/DataLabels.csv', sep=',', header=None)\n",
    "dataLabels = np.array(dataLabels)\n",
    "# data = pd.read_csv('../Project/features.csv', sep=',', header=None)\n",
    "# labels = pd.read_csv('../Project/labels.csv', sep=',', header=None)\n",
    "\n",
    "y = labels\n",
    "y = np.array(y)\n",
    "x = data\n",
    "Cats, y = np.unique(y, return_inverse = True)\n",
    "# ySub = (y <10) #Toggle for faster computations when writing and testing algorithms\n",
    "# x= x[ySub]\n",
    "# y = y[ySub]\n",
    "xMean = np.mean(x, axis = 0)\n",
    "xStd = np.std(x, axis = 0)\n",
    "testFeatures = (testFeatures - xMean)/xStd\n",
    "x = (x - np.mean(x, axis =0))/(np.std(x,axis = 0))\n",
    "x = np.array(x)\n",
    "\n",
    "#Thanks to scikit learn documentation for demonstrating this way to extract a training set with same proportions of samples in test and train set as in original data \n",
    "def splitTestTrain(x, y, testPCT, numSplits):\n",
    "    sss = StratifiedShuffleSplit(n_splits=numSplits, test_size=testPCT, random_state=0)\n",
    "    for train_index, test_index in sss.split(x,y):\n",
    "        xTrain, xTest = x[train_index], x[test_index]\n",
    "        yTrain, yTest = y[train_index], y[test_index]\n",
    "        return (xTrain, xTest,yTrain, yTest)\n",
    "\n",
    "xTrain, xTest,yTrain, yTest = splitTestTrain(x,y,.2, 1)\n",
    "beta_init = np.zeros(x.shape[1])\n",
    "testingBetas = np.random.randint(-50,50, size = x.shape[1])/50\n",
    "\n",
    "def predstoFile(preds, filename):\n",
    "    newpreds = convertY(preds, Cats, dataLabels) #Converts predictions using my labels to predictions with the standard labels\n",
    "    LabsPreds = np.stack((testLabels,newpreds)).T # links predictions to corresponding image label\n",
    "    LabsPreds = LabsPreds.astype(int) #converts to integer type for sorting\n",
    "    LabsPreds = LabsPreds[np.argsort(LabsPreds[:, 0])]\n",
    "    names = np.array(['Id', 'Prediction'])\n",
    "    LabsPreds = np.vstack((names, LabsPreds))\n",
    "    np.savetxt(filename, LabsPreds, delimiter = \",\", fmt='%s')\n",
    "    return\n",
    "\n",
    "def convertY(preds, Cats = Cats, labels = dataLabels):\n",
    "    #preds is my predictions based on my categories\n",
    "    #Cats is the link back to the name of the bird from my category\n",
    "    #Labels is the correct bird / number mapping as defined by the contest organizer\n",
    "    namePreds = Cats[preds]\n",
    "    numPreds = ()\n",
    "    namPredsCheck = ()\n",
    "    \n",
    "    for i in range(len(preds)):\n",
    "        newCatPred = labels[np.where(labels[:,1] == Cats[preds][i])[0],0]  \n",
    "        newNamePred = labels[np.where(labels[:,1] == Cats[preds][i])[0],1]\n",
    "        #print(i,newCatPred,newNamePred)\n",
    "        numPreds = np.append(numPreds, newCatPred)\n",
    "        namPredsCheck = np.append(namPredsCheck, newNamePred)\n",
    "    return(numPreds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computegrad(lam,betas, y=yTrain, x= xTrain): #Thanks to Rajiv for explaining this\n",
    "    n = len(x)\n",
    "    xdotb = x.dot(betas)\n",
    "    denom = 1/( 1+np.exp( y*xdotb ))\n",
    "    grad=-x.T.dot(y*denom)\n",
    "    grad = (grad/n)+lam*2*betas\n",
    "    return(grad)\n",
    "# def computegrad(betas , lam , x=xTrain , y=yTrain): \n",
    "#     yx = y[:, np.newaxis]*x \n",
    "#     denom = 1+np.exp(-yx.dot(betas)) \n",
    "#     grad = 1/len(y)*np.sum(-yx*np.exp(-yx.dot(betas[:,np.newaxis]))/denom[:,np.newaxis] , axis=0) + 2 *lam*betas\n",
    "#     return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objfunc(lam, y, x, betas):\n",
    "    obj = (np.sum(np.log(1+np.exp(-y*x.dot(betas)))))/x.shape[0] + lam * np.linalg.norm(betas)**2\n",
    "    return(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialize Backtracking function\n",
    "def backtracking(lam, betas,x, y, t = 10, alpha = 0.5, gamma = 0.8, maxIter = 100):\n",
    "    # f = objective function\n",
    "    # grad = gradient function\n",
    "    # betas = current point\n",
    "    # t = starting step size\n",
    "    # alpha = constant used to define sufficient decrease condition\n",
    "    # gamma = fraction by which we drecrease t if the previous T doesn't work\n",
    "    # maxIter = maximume iterations for algorithm\n",
    "    #output = t, the step size to use\n",
    "    # Set inital step size. What is it See page 25 in week 3 notes?\n",
    "    grad_b = computegrad(betas = betas, lam = lam , x=x, y=y)\n",
    "    norm_grad_b = np.linalg.norm(grad_b)\n",
    "    found_t = 0\n",
    "    iters = 0\n",
    "    t = deepcopy(t)\n",
    "    while found_t == 0 and iters < maxIter:\n",
    "        if objfunc(betas = (betas-t*grad_b), lam = lam, x=x, y=y) < objfunc(betas = betas, lam = lam , x=x , y=y)-alpha*t*norm_grad_b**2:\n",
    "            found_t = 1 \n",
    "        else:\n",
    "            t *= gamma\n",
    "            iters += 1\n",
    "            #print(\"t =:\",str(t), \"lhs =:\", str(lhs), \"rhs =\", str(rhs))\n",
    "\n",
    "    return(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import eigh as largest_eigh\n",
    "#Caluclate initial step size (page 28 in week 3 notes)\n",
    "def calcStepInit (x = xTrain, y = yTrain, lam = 1):\n",
    "    n = x.shape[0]\n",
    "    n1 =  x.shape[1]\n",
    "    MaxEigVal = largest_eigh(np.dot(1/n*x.T, x), eigvals= (n1-1, n1-1))[0]\n",
    "    L = MaxEigVal + lam\n",
    "    StepInit = (1/(MaxEigVal + lam))\n",
    "    StepInit = float(StepInit)\n",
    "    return(StepInit)\n",
    "StepInit = calcStepInit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fastgradalgo(betas, theta, step, x,  y, lam, maxIter = 1000):\n",
    "\n",
    "    # betas = initial beta\n",
    "    # theta = initial theta\n",
    "    # alpha = constant used to define sufficient decrease condition\n",
    "    # beta = fraction by which we drecrease t if the previous T doesn't work\n",
    "    # maxIter = maximume iterations for algorithm\n",
    "    # output = series of betas, the step size to use\n",
    "    theta = np.zeros(xTrain.shape[1])\n",
    "    b_vals = [betas]\n",
    "    for i in range(0,maxIter):\n",
    "        step = backtracking(betas = betas, t = step, lam = lam, x = xTrain, y = yTrain)\n",
    "        #print(\"i =\", i)\n",
    "        #print(\"step =\", step)\n",
    "        betaNew = theta - step*computegrad(betas = theta, lam = lam, x = x, y = y)\n",
    "        #print(b)\n",
    "        b_vals.append(betaNew) \n",
    "        #print(b_vals)\n",
    "        theta = betaNew+ (i/(i+3))*(betaNew - betas)\n",
    "        betas = betaNew\n",
    "    return(b_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute optimal betas\n",
    "myLam = 1\n",
    "fgBetas = fastgradalgo(betas = beta_init, theta = beta_init,step = StepInit, x = xTrain, y=yTrain,lam = myLam,  maxIter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create function to calculate objective values for array of betas\n",
    "def calcObjVal(betas, lam, x, y):\n",
    "    objVals = []\n",
    "    for i in range(0,len(betas)):\n",
    "        objVals.append(objfunc(betas = betas[i], lam = lam, x = x, y = y))\n",
    "    return(objVals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x29c983587f0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntKEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29c97db5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fgObjVals = calcObjVal(betas = fgBetas, lam = 1, x = xTrain, y = yTrain)\n",
    "%matplotlib inline\n",
    "plt.plot(range(len(fgObjVals)), fgObjVals, label = \"Fast Grad Algo\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Value of Objective function\")\n",
    "\n",
    "plt.title(\"Objective value vs iterations of Fast Grad Algorithm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def class_error (betaStar, x , y): \n",
    "    pred = 1/(1+np.exp(-x.dot(betaStar))) > 0.5 \n",
    "    pred = pred*2- 1 # Convert to +/− 1 \n",
    "    error = np.mean(pred !=y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_error_array(betas, x, y):\n",
    "    n = len(betas)\n",
    "    errors = ()\n",
    "    for i in range(n):\n",
    "        error = class_error(betas[i], x, y)\n",
    "        errors = np.append(errors, error)\n",
    "    return(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x19570647e48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuziRmJlZuziRmJlZu/x/zDkcN8sbkhkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19570610cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TrainErrors = class_error_array(betas = fgBetas, x= xTrain, y = yTrain)\n",
    "TestErrors = class_error_array(betas = fgBetas, x= xTest, y = yTest)\n",
    "\n",
    "plt.plot(TrainErrors,label = \"Train set\")\n",
    "plt.plot(TestErrors, label = \"Test set\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.title(\"Misclassification error rate vs iteration of fast grad algorithm\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Classification Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bullet 2**  \n",
    "Find the value of the regularization parameter λ using cross-validation; you may use\n",
    "scikit-learn’s built-in functions for this purpose. Train an L2-regularized logistic regression classifier on the training set using your own fast gradient algorithm with that value of λ found by cross-validation. Plot, with different colors, the misclassification error on the training set and on the validation set vs iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal lambda for cross validation is: 0.00075\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEaCAYAAAA7YdFPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19570671be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cs = (.0000001, .000001,.00001, .0001,.001, .01, .1, 1, 10, 100, 1000, 10000, 100000)\n",
    "MyLogCV = LogisticRegressionCV(penalty='l2', Cs = cs, fit_intercept=False, tol=10e-8, max_iter=1000) \n",
    "MyLogCV.fit(x , y) \n",
    "BestLam = MyLogCV.C_[0] \n",
    "AdjLam = 1/(BestLam*2*x.shape[0])\n",
    "AdjLam = (BestLam*x.shape[0]/2)\n",
    "\n",
    "print(\"The optimal lambda for cross validation is:\",AdjLam)\n",
    "coeff = MyLogCV.coef_==0\n",
    "errors = 1- (np.mean(MyLogCV.scores_[9], axis = 0))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cs,errors, 'm-')\n",
    "plt.xlabel('C values tested')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.title('Misclassification error vs C in cross validation')\n",
    "\n",
    "ax.plot()\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x19570617588>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19570c35630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myLam = AdjLam\n",
    "fgBetas = fastgradalgo(betas = beta_init, theta = beta_init, x = xTrain, y = yTrain, step = StepInit, maxIter = 1000, lam = myLam)\n",
    "TrainErrors = class_error_array(betas = fgBetas, x= xTrain, y = yTrain)\n",
    "TestErrors = class_error_array(betas = fgBetas, x= xTest, y = yTest)\n",
    "plt.plot(TrainErrors,label = \"Train set\")\n",
    "plt.plot(TestErrors, label = \"Test set\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.title(\"Misclassification error rate vs iteration of fast grad algorithm\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Classification Error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW5 Bullet 1**  \n",
    "Pick k = 5 classes of your choice from the dataset. You may choose any subset of 5 classes among all classes of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y = labels\n",
    "y = np.array(y)\n",
    "x = data\n",
    "Cats, y = np.unique(y, return_inverse = True)\n",
    "ySub = ((y == 139) + (y == 138)+(y == 137) + (y == 136)+(y == 135))\n",
    "x= x[ySub]\n",
    "y = y[ySub]\n",
    "x = (x - np.mean(x, axis =0))/(np.std(x,axis = 0))\n",
    "x = np.array(x)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "def splitTestTrain(x, y, testPCT, numSplits):\n",
    "    sss = StratifiedShuffleSplit(n_splits=numSplits, test_size=testPCT, random_state=0)\n",
    "    for train_index, test_index in sss.split(x,y):\n",
    "        xTrain, xTest = x[train_index], x[test_index]\n",
    "        yTrain, yTest = y[train_index], y[test_index]\n",
    "        return (xTrain, xTest,yTrain, yTest)\n",
    "\n",
    "xTrain, xTest,yTrain, yTest = splitTestTrain(x,y,.2, 1)\n",
    "beta_init = np.zeros(x.shape[1])\n",
    "testingBetas = np.random.randint(-50,50, size = x.shape[1])/50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW5 Bullet 2**  \n",
    "Write a function that, for any class at hand, creates a training set with an equal number of examples from the class at hand and from the other classes. You may simply randomly pick the examples from the other classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subset_data(x, y, c, trainPCT):\n",
    "    #x is the dataset with features\n",
    "    #y is the dataset with labels\n",
    "    #C is the category that was chosen\n",
    "    #trainPCT is the percent of the dataset you want to be training data\n",
    "    if sum(y == c) == 0: \n",
    "            raise ('Category selected does not exist in this data') \n",
    "    else:\n",
    "        ysubc = (y==c)\n",
    "        notc = [not i for i in ysubc]\n",
    "        yc, xc, ynotc, xnotc = y[ysubc], x[ysubc], y[notc], x[notc]\n",
    "        n = round(len(yc) * trainPCT) #Lenght of training set\n",
    "        nt =len(yc) - n #length of test set\n",
    "        cSelector = (np.append([[True]*n], [(len(yc)-n)*[False]]))\n",
    "        notCSelector = (np.append([[True]*n], [(len(ynotc)-n)*[False]]))\n",
    "        np.random.shuffle(cSelector)\n",
    "        np.random.shuffle(notCSelector)\n",
    "\n",
    "        yTrain = np.append([yc[cSelector]],[ynotc[notCSelector]])\n",
    "        yTrainBin = deepcopy(yTrain)\n",
    "        yTrainBin[yTrain!=c], yTrainBin[yTrain==c] = -1, 1\n",
    "        \n",
    "        xTrain = np.vstack((xc[cSelector],xnotc[notCSelector]))\n",
    "        yTest = np.append([yc[~cSelector]],[ynotc[notCSelector][:nt]])\n",
    "        yTestBin = deepcopy(yTest)\n",
    "        yTestBin[yTestBin!=c], yTestBin[yTestBin==c] = -1, 1\n",
    "\n",
    "        xTest = np.vstack((xc[~cSelector],xnotc[notCSelector][:nt,:]))\n",
    "\n",
    "    return(xTrain, yTrain,yTrainBin, xTest, yTest, yTestBin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW5 Bullet 3**  \n",
    "For each class c, train an L2-regularized logistic regression classiﬁer using your own fast gradient algorithm with λc = 1. Display the confusion matrix. Which classes seem to be the most diﬃcult to classify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def confusionMatrix(x, y, lam, trainPCT = .75):\n",
    "    Cs = np.unique(y)\n",
    "    nt =int(len(y)/len(Cs) - round(len(y)/len(Cs) * trainPCT))*2 #length of test set \n",
    "    ConfMat = np.zeros((len(Cs),len(Cs))) #initialize repository for confidence matrix\n",
    "    #initialize dictionaries to store the data sets \n",
    "    Preds = np.zeros((len(y), len(Cs)))\n",
    "\n",
    "    myBetas={}\n",
    "    for i in range(len(Cs)):\n",
    "        c = Cs[i]\n",
    "        #ConfMat[0,i] = c #label axes of confusion matrix\n",
    "        #ConfMat[i,0] = c\n",
    " \n",
    "        #generate data\n",
    "        xTrainTemp, yTrainTemp,yTrainBinTemp, xTestTemp, yTestTemp, yTestBinTemp = subset_data(x = x, y = y, c = c, trainPCT = trainPCT)\n",
    "        \n",
    "        #Calculate step size\n",
    "        stepInit = calcStepInit (x = xTrainTemp, y = yTrainTemp, lam = lam)\n",
    "       \n",
    "        #Train model\n",
    "        myBetas[c] = fastgradalgo(x=xTrainTemp, y = yTrainBinTemp, betas = np.zeros(x.shape[1]), theta = np.zeros(x.shape[1]),  step = stepInit, lam = lam, maxIter = 100)[-1]\n",
    "    \n",
    "    #Find the prediction using each set of betas\n",
    "    for i in range(len(Cs)):\n",
    "        c = Cs[i]\n",
    "        #Find predictions for our test set\n",
    "        pred = 1/(1+np.exp(-x.dot(myBetas[c]))) \n",
    "        Preds[:,i] =deepcopy(pred) #Capture predictions for each run through data\n",
    "   \n",
    "    #Select prediction with highest probability and create the confusion matrix\n",
    "    MaxProbIndex = np.argmax(Preds, axis = 1)\n",
    "    for k in range(len(y)):\n",
    "        column = MaxProbIndex[k]\n",
    "        row = np.where(Cs == y[k])[0]\n",
    "        ConfMat[row, column] = ConfMat[row, column] + 1\n",
    "            #ConfMat[np.where(Cs == Preds[k])[0],np.where(Cs == YTest[k])[0]] = ConfMat[np.where(Cs == Preds[k])[0],np.where(Cs == YTest[k])[0]]+ 1\n",
    "    \n",
    "    ConfMat = pd.DataFrame(data=ConfMat, index=Cs, columns=Cs)\n",
    "    preds = Cs[MaxProbIndex]\n",
    "    MultiClassError = np.mean(preds != y)\n",
    "    return (ConfMat, preds, MultiClassError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      135   136   137   138   139\n",
      "135  30.0   0.0   0.0   0.0   0.0\n",
      "136   0.0  29.0   1.0   0.0   0.0\n",
      "137   0.0   1.0  29.0   0.0   0.0\n",
      "138   0.0   0.0   0.0  30.0   0.0\n",
      "139   0.0   0.0   0.0   0.0  30.0\n",
      "[139 139 136 138 136 136 138 138 138 139 135 138 135 137 138 136 137 139\n",
      " 138 139 139 137 136 137 139 139 137 135 139 135 135 135 139 135 135 137\n",
      " 137 135 139 136 135 138 135 136 137 136 139 137 138 138 139 135 139 137\n",
      " 135 136 137 136 135 135 137 137 135 137 139 138 138 136 136 136 135 137\n",
      " 136 138 136 135 136 138 139 138 136 138 137 137 135 137 139 138 138 137\n",
      " 135 136 139 137 138 139 136 136 139 138 139 138 135 135 137 139 137 139\n",
      " 137 136 137 136 137 138 138 138 136 137 137 139 135 137 138 138 139 135\n",
      " 135 136 138 138 137 139 139 136 135 135 139 136 136 138 137 135 139 136\n",
      " 138 135 136 135 139 136]\n",
      "0.0133333333333\n"
     ]
    }
   ],
   "source": [
    "CMat, Preds, Error=confusionMatrix(x=x, y=y, lam=1, trainPCT = .75)\n",
    "print(CMat)\n",
    "print(Preds)\n",
    "print(Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW5 Bullet 4**  \n",
    "Write a function that returns the ranked list of classes in terms of classiﬁcation diﬃculty using the confusion matrix. Compute the multi-class misclassiﬁcation error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classDiff(CM):\n",
    "    #CM = confusion matrix\n",
    "    #DataFrame.sum(axis=None, skipna=None, level=None, numeric_only=None, **kwargs)\n",
    "    size = CM.shape[0]\n",
    "    difficulty = pd.DataFrame.sum(CM, axis = 0)\n",
    "    for i in range(0,size):\n",
    "        difficulty.iloc[i] = difficulty.iloc[i] - CM.iloc[i,i]\n",
    "    difficulty.sort_values(inplace = True, ascending = False)\n",
    "    MisClassError = np.sum(difficulty.sum())/np.sum(CM.sum())\n",
    "    return(difficulty, MisClassError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137    1.0\n",
      "136    1.0\n",
      "139    0.0\n",
      "138    0.0\n",
      "135    0.0\n",
      "dtype: float64\n",
      "0.0133333333333\n"
     ]
    }
   ],
   "source": [
    "a, b = classDiff(CMat)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW5 Bullet 5**  \n",
    "Find the values of the regularization parameters λ1,...,λk for the classiﬁers using a hold-out validation set strategy. Deﬁne a grid of values Λ for each parameter λc with c = 1,...,k. For each setting of the regularization parameters λ1,...,λk, where each λc can take values in Λ (independently), train all your k = 5 classiﬁers and save the multi-class misclassiﬁcation error on the validation set for each setting of the regularization parameters λ1,...,λk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = labels\n",
    "y = np.array(y)\n",
    "x = data\n",
    "Cats, y = np.unique(y, return_inverse = True)\n",
    "Bird = 'Wren' # choose a species of bird with several subspecies\n",
    "Indices = [i for i, v in enumerate(Cats) if Bird in v] #Find indices that include species of bird\n",
    "Indices = Indices[0:5]\n",
    "ySub = ((y == Indices[0])+ (y == Indices[1])+(y == Indices[2])+ (y == Indices[3])+(y == Indices[4]))\n",
    "x= x[ySub]\n",
    "y = y[ySub]\n",
    "x = (x - np.mean(x, axis =0))/(np.std(x,axis = 0))\n",
    "x = np.array(x)\n",
    "xTrain, xTest,yTrain, yTest = splitTestTrain(x,y,.2, 1)\n",
    "beta_init = np.zeros(x.shape[1])\n",
    "testingBetas = np.random.randint(-50,50, size = x.shape[1])/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This data subset assumes test and train have already been split out\n",
    "def subset_data2(x, y, c):\n",
    "    #x is the dataset with features\n",
    "    #y is the dataset with labels\n",
    "    #C is the category that was chosen\n",
    "    if sum(y == c) == 0: \n",
    "            raise ('Category selected does not exist in this data') \n",
    "    else:\n",
    "        ysubc = (y==c)\n",
    "        notc = [not i for i in ysubc]\n",
    "        yc, xc, ynotc, xnotc = y[ysubc], x[ysubc], y[notc], x[notc]\n",
    "        n = len(yc) #Lenght of things to select from the not c set\n",
    "        notCSelector = (np.append([[True]*n], [(len(ynotc)-n)*[False]]))\n",
    "        np.random.shuffle(notCSelector)\n",
    "\n",
    "        yTemp = np.append([yc],[ynotc[notCSelector]])\n",
    "        yTempBin = deepcopy(yTemp)\n",
    "        yTempBin[yTemp!=c], yTempBin[yTemp==c] = -1, 1\n",
    "        \n",
    "        xTemp = np.vstack((xc,xnotc[notCSelector]))\n",
    "       \n",
    "    return(xTemp, yTemp,yTempBin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialize a function that returns a probability of c or not c for each image, given a category c and each value of lambda\n",
    "\n",
    "def findprobs(c, lambdas, x = xTrain, y = yTrain, xTest = xTest):\n",
    "    nt =len(yTest) #length of test set \n",
    "    Preds = np.zeros((len(lambdas), nt))\n",
    "    myBetas={}\n",
    "    \n",
    "    #generate data\n",
    "    xTemp, yTemp,yTempBin = subset_data2(x = x, y = y, c = c)\n",
    "        \n",
    "    #Calculate step size\n",
    "       \n",
    "    for i in range(len(lambdas)):\n",
    "        lam = lambdas[i]\n",
    "        stepInit = calcStepInit (x = xTemp, y = yTemp, lam = lam)\n",
    "\n",
    "  \n",
    "        #Train model\n",
    "        myBetas = fastgradalgo(x=xTemp, y = yTempBin, betas = np.zeros(x.shape[1]), theta = np.zeros(x.shape[1]),  step = stepInit, lam = lam, maxIter = 100)[-1]\n",
    "    \n",
    "        pred = 1/(1+np.exp(-xTest.dot(myBetas))) \n",
    "        Preds[i,:] = pred\n",
    "    return(Preds)\n",
    "     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lams = np.array((.001, .01, .1, 1, 10, 100, 1000))\n",
    "\n",
    "def findOptLams(xTest = xTest, yTest = yTest, xTrain = xTrain, yTrain = yTrain, lambdas = lams):\n",
    "    #Initialize variables we will use in the function\n",
    "    nLams = len(lams)\n",
    "    Cs = np.unique(y)\n",
    "    Probs = np.zeros((len(Cs,), nLams, len(yTest))) # repository to store all predicted probabilities\n",
    "    TempProbs = np.zeros((len(yTest), len(Cs,))) # repository to store temprorary probabilities for comparison using each combindation of lambdas\n",
    "    columns = ('C1 Lamda','C2 Lamda','C3 Lamda','C4 Lamda','C5 Lamda','Error')\n",
    "    ErrorTrack = pd.DataFrame(columns = columns)\n",
    "    yCats = np.unique(yTest)\n",
    "    #fit models and create predictions  probabilities for \n",
    "    for i in range(len(Cs)):\n",
    "        c = Cs[i]\n",
    "        CProbs = findprobs(c = c, lambdas = lambdas, x = xTrain, y = yTrain, xTest = xTest) # Create predicted probabilities using 1vr for each category using each lambda\n",
    "        Probs[i,:,:] = CProbs\n",
    "    \n",
    "    # find prediction for each level of lambda in each category\n",
    "    for i in range(nLams):\n",
    "        c1 = Cs[0]\n",
    "        lam1 = lambdas[i]\n",
    "        TempProbs[:, 0] = deepcopy(Probs[0, i, :])\n",
    "        for j in range(nLams):\n",
    "            c2 = Cs[1]\n",
    "            lam2 = lambdas[j]\n",
    "            TempProbs[:, 1] = deepcopy(Probs[1, j, :])\n",
    "            for k in range(nLams):\n",
    "                c3 = Cs[2]\n",
    "                lam3 = lambdas[k]\n",
    "                TempProbs[:, 2] = deepcopy(Probs[2, k, :])            \n",
    "                for l in range(nLams):\n",
    "                    c4 = Cs[3]\n",
    "                    lam4 = lambdas[l]\n",
    "                    TempProbs[:, 3] = deepcopy(Probs[3, l, :])\n",
    "                    for m in range(nLams):\n",
    "                        c5 = Cs[4]\n",
    "                        lam5 = lambdas[m]\n",
    "                        TempProbs[:, 4] = deepcopy(Probs[4, m, :])\n",
    "                        MaxProbIndex = np.argmax(TempProbs, axis = 1)\n",
    "                        Pred = yCats[MaxProbIndex]\n",
    "                        #print(\"Pred:\", Pred, \"YTest:\", yTest )\n",
    "                        Error = 1 - np.mean(Pred == yTest)\n",
    "                        tempError = pd.DataFrame([[lam1, lam2, lam3, lam4, lam5, Error]], columns = columns)\n",
    "                        ErrorTrack = ErrorTrack.append(tempError)#            \n",
    "    return(ErrorTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alyssa\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1 Lamda</th>\n",
       "      <th>C2 Lamda</th>\n",
       "      <th>C3 Lamda</th>\n",
       "      <th>C4 Lamda</th>\n",
       "      <th>C5 Lamda</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    C1 Lamda  C2 Lamda  C3 Lamda  C4 Lamda  C5 Lamda     Error\n",
       "0      100.0     0.001     0.001     0.001     100.0  0.210526\n",
       "0      100.0     0.001     0.001     0.010     100.0  0.210526\n",
       "0      100.0     0.001     0.001     0.100     100.0  0.210526\n",
       "0      100.0     0.001     0.001     1.000     100.0  0.210526\n",
       "0      100.0     0.001     0.001    10.000     100.0  0.210526\n",
       "0      100.0     0.001     0.010     0.001     100.0  0.210526\n",
       "0      100.0     0.001     0.010     0.010     100.0  0.210526\n",
       "0      100.0     0.001     0.010     0.100     100.0  0.210526\n",
       "0      100.0     0.001     0.010     1.000     100.0  0.210526\n",
       "0      100.0     0.001     0.010    10.000     100.0  0.210526\n",
       "0      100.0     0.001     0.100     0.001     100.0  0.210526\n",
       "0      100.0     0.001     0.100     0.010     100.0  0.210526\n",
       "0      100.0     0.001     0.100     0.100     100.0  0.210526\n",
       "0      100.0     0.001     0.100     1.000     100.0  0.210526\n",
       "0      100.0     0.001     0.100    10.000     100.0  0.210526\n",
       "0      100.0     0.001     1.000     0.001     100.0  0.210526\n",
       "0      100.0     0.001     1.000     0.010     100.0  0.210526\n",
       "0      100.0     0.001     1.000     0.100     100.0  0.210526\n",
       "0      100.0     0.001     1.000     1.000     100.0  0.210526\n",
       "0      100.0     0.001     1.000    10.000     100.0  0.210526\n",
       "0      100.0     0.001   100.000     0.001     100.0  0.210526\n",
       "0      100.0     0.001   100.000     0.010     100.0  0.210526\n",
       "0      100.0     0.001   100.000     0.100     100.0  0.210526\n",
       "0      100.0     0.001   100.000     1.000     100.0  0.210526\n",
       "0      100.0     0.001   100.000    10.000     100.0  0.210526\n",
       "0      100.0     0.001   100.000   100.000     100.0  0.210526\n",
       "0      100.0     0.010     0.001     0.001     100.0  0.210526\n",
       "0      100.0     0.010     0.001     0.010     100.0  0.210526\n",
       "0      100.0     0.010     0.001     0.100     100.0  0.210526\n",
       "0      100.0     0.010     0.001     1.000     100.0  0.210526\n",
       "..       ...       ...       ...       ...       ...       ...\n",
       "0      100.0   100.000     0.001     0.100     100.0  0.210526\n",
       "0      100.0   100.000     0.001     1.000     100.0  0.210526\n",
       "0      100.0   100.000     0.001    10.000     100.0  0.210526\n",
       "0      100.0   100.000     0.010     0.001     100.0  0.210526\n",
       "0      100.0   100.000     0.010     0.010     100.0  0.210526\n",
       "0      100.0   100.000     0.010     0.100     100.0  0.210526\n",
       "0      100.0   100.000     0.010     1.000     100.0  0.210526\n",
       "0      100.0   100.000     0.010    10.000     100.0  0.210526\n",
       "0      100.0   100.000     0.100     0.001     100.0  0.210526\n",
       "0      100.0   100.000     0.100     0.010     100.0  0.210526\n",
       "0      100.0   100.000     0.100     0.100     100.0  0.210526\n",
       "0      100.0   100.000     0.100     1.000     100.0  0.210526\n",
       "0      100.0   100.000     0.100    10.000     100.0  0.210526\n",
       "0      100.0   100.000     1.000     0.001     100.0  0.210526\n",
       "0      100.0   100.000     1.000     0.010     100.0  0.210526\n",
       "0      100.0   100.000     1.000     0.100     100.0  0.210526\n",
       "0      100.0   100.000     1.000     1.000     100.0  0.210526\n",
       "0      100.0   100.000     1.000    10.000     100.0  0.210526\n",
       "0      100.0   100.000    10.000     0.001     100.0  0.210526\n",
       "0      100.0   100.000    10.000     0.010     100.0  0.210526\n",
       "0      100.0   100.000    10.000     0.100     100.0  0.210526\n",
       "0      100.0   100.000    10.000     1.000     100.0  0.210526\n",
       "0      100.0   100.000    10.000    10.000     100.0  0.210526\n",
       "0      100.0   100.000    10.000   100.000     100.0  0.210526\n",
       "0      100.0   100.000   100.000     0.001     100.0  0.210526\n",
       "0      100.0   100.000   100.000     0.010     100.0  0.210526\n",
       "0      100.0   100.000   100.000     0.100     100.0  0.210526\n",
       "0      100.0   100.000   100.000     1.000     100.0  0.210526\n",
       "0      100.0   100.000   100.000    10.000     100.0  0.210526\n",
       "0      100.0   100.000   100.000   100.000     100.0  0.210526\n",
       "\n",
       "[168 rows x 6 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show all combinations of lambda that minmize the misclassification error\n",
    "AllLams = findOptLams(xTest = xTest, yTest = yTest, xTrain = xTrain, yTrain = yTrain, lambdas = lams)\n",
    "MinError = AllLams.min(axis=0).iloc[5]\n",
    "OptLams = AllLams.loc[AllLams['Error'] == MinError]\n",
    "OptLams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW5 Bullet 6**  \n",
    "Find the optimal value of the regularization parameters λ1,...,λk based on the validation error. Display the confusion matrix for this setting of the regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 100.,   10.,  100.,  100.,  100.])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(OptLams.iloc[-1, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to find predictions for a given category c and a given lambda l\n",
    "def findprobsOneLam(c, lam, x = xTrain, y = yTrain, xTest = xTest):\n",
    "    nt =len(yTest) #length of test set \n",
    "    myBetas={}\n",
    "    \n",
    "    #generate data\n",
    "    xTemp, yTemp,yTempBin = subset_data2(x = x, y = y, c = c)\n",
    "        \n",
    "    #Calculate step size\n",
    "       \n",
    "    stepInit = calcStepInit (x = xTemp, y = yTemp, lam = lam)\n",
    "\n",
    "  \n",
    "        #Train model\n",
    "    myBetas = fastgradalgo(x=xTemp, y = yTempBin, betas = np.zeros(x.shape[1]), theta = np.zeros(x.shape[1]),  step = stepInit, lam = lam, maxIter = 100)[-1]\n",
    "    \n",
    "    pred = 1/(1+np.exp(-xTest.dot(myBetas))) \n",
    "    return(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a confusion matrix with optimal lambdas\n",
    "myOptLams = np.array(OptLams.iloc[-1, :5])\n",
    "\n",
    "def OptLamConfMat(xTest = xTest, yTest = yTest, xTrain = xTrain, yTrain = yTrain, Lams = myOptLams):\n",
    "    #Initialize variables we will use in the function\n",
    "    #Lams is an array of lambdas of length of the number of categories where each entry is the optimal lambda for a category (listed in same order as category)\n",
    "    nLams = len(Lams)\n",
    "    Cs = np.unique(y)\n",
    "    Probs = np.zeros((len(yTest), len(Cs,))) # repository to store all predicted probabilities\n",
    "    yCats = np.unique(yTest)\n",
    "    ConfMat = np.zeros((len(Cs),len(Cs))) #initialize repository for confidence matrix\n",
    "\n",
    "    #fit models and create predictions  probabilities for \n",
    "    for i in range(len(Cs)):\n",
    "        c = Cs[i]\n",
    "        lam = Lams[i]\n",
    "        CProbs = findprobsOneLam(c = c, lam = lam, x = xTrain, y = yTrain, xTest = xTest) # Create predicted probabilities using 1vr for each category using each lambda\n",
    "        Probs[:,i] = CProbs\n",
    "        \n",
    "    MaxProbIndex = np.argmax(Probs, axis = 1)\n",
    "    for k in range(len(yTest)):\n",
    "        column = MaxProbIndex[k]\n",
    "        row = np.where(Cs == yTest[k])[0]\n",
    "        ConfMat[row, column] = ConfMat[row, column] + 1\n",
    "            #ConfMat[np.where(Cs == Preds[k])[0],np.where(Cs == YTest[k])[0]] = ConfMat[np.where(Cs == Preds[k])[0],np.where(Cs == YTest[k])[0]]+ 1\n",
    "#     ConfMat = pd.DataFrame(data=ConfMat, index=Cats[Cs], columns=Cats[Cs])\n",
    "    preds = Cs[MaxProbIndex]\n",
    "    MultiClassError = np.mean(preds != yTest)\n",
    "    return(ConfMat, MultiClassError,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bewick_Wren</th>\n",
       "      <th>Cactus_Wren</th>\n",
       "      <th>Carolina_Wren</th>\n",
       "      <th>Marsh_Wren</th>\n",
       "      <th>Rock_Wren</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bewick_Wren</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cactus_Wren</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carolina_Wren</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marsh_Wren</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rock_Wren</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Bewick_Wren  Cactus_Wren  Carolina_Wren  Marsh_Wren  Rock_Wren\n",
       "Bewick_Wren            5.0          0.0            1.0         0.0        0.0\n",
       "Cactus_Wren            1.0          5.0            0.0         0.0        0.0\n",
       "Carolina_Wren          1.0          0.0            5.0         0.0        0.0\n",
       "Marsh_Wren             1.0          0.0            1.0         4.0        0.0\n",
       "Rock_Wren              1.0          0.0            1.0         0.0        4.0"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OptLamConfMat(xTest = xTest, yTest = yTest, xTrain = xTrain, yTrain = yTrain, Lams = myOptLams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extending my one vs rest from five categories to all categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Redefine x and y as my whole dataset\n",
    "y = labels\n",
    "y = np.array(y)\n",
    "x = data\n",
    "Cats, y = np.unique(y, return_inverse = True)\n",
    "x = (x - np.mean(x, axis =0))/(np.std(x,axis = 0))\n",
    "x = np.array(x)\n",
    "xTrain, xTest,yTrain, yTest = splitTestTrain(x,y,.2, 1)\n",
    "beta_init = np.zeros(x.shape[1])\n",
    "testingBetas = np.random.randint(-50,50, size = x.shape[1])/50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This data subset assumes test and train have already been split out, and makes a set that has equal number from chosen set C  and all other sets combined. This is achieved by resampling C\n",
    "def subset_dataAllC(x, y, c):\n",
    "    #x is the dataset with features\n",
    "    #y is the dataset with labels\n",
    "    #C is the category that was chosen\n",
    "    if sum(y == c) == 0: \n",
    "            raise ('Category selected does not exist in this data') \n",
    "    else:\n",
    "        ysubc = (y==c)\n",
    "        notc = [not i for i in ysubc]\n",
    "        yc, xc, ynotc, xnotc = y[ysubc], x[ysubc], y[notc], x[notc]\n",
    "        n = int(len(ynotc)/len(yc)) #number of copies of C we need to add to make a balanced dataset\n",
    "        yTemp = ynotc\n",
    "        xTemp = xnotc\n",
    "        for i in range(n):\n",
    "            yTemp = np.append(yTemp, yc)\n",
    "            xTemp = np.vstack((xTemp, xc))\n",
    "\n",
    "        yTempBin = deepcopy(yTemp)\n",
    "        yTempBin[yTemp!=c], yTempBin[yTemp==c] = -1, 1\n",
    "               \n",
    "    return(xTemp, yTemp,yTempBin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since there are too many combinations of lambda to test each combinatin, let's see what kind of accuracy we can get if we use all the same lambda\n",
    "def confusionMatrixBig(xTrain, yTrain, xTest, yTest, lam):\n",
    "    Cs = np.unique(y)\n",
    "    ConfMat = np.zeros((len(Cs),len(Cs))) #initialize repository for confidence matrix\n",
    "    #initialize dictionaries to store the data sets \n",
    "    Preds = np.zeros((len(yTest), len(Cs)))\n",
    "\n",
    "    myBetas={}\n",
    "    for i in range(len(Cs)):\n",
    "        c = Cs[i]\n",
    "        #ConfMat[0,i] = c #label axes of confusion matrix\n",
    "        #ConfMat[i,0] = c\n",
    " \n",
    "        #generate data\n",
    "        xTrainTemp, yTrainTemp,yTrainBinTemp = subset_dataAllC(x = xTrain, y = yTrain, c = c)\n",
    "        yTestBinTemp = (yTest == c) *2 -1\n",
    "        \n",
    "        #Calculate step size\n",
    "        stepInit = calcStepInit (x = xTrainTemp, y = yTrainTemp, lam = lam)\n",
    "       \n",
    "        #Train model\n",
    "        TempBetas = fastgradalgo(x=xTrainTemp, y = yTrainBinTemp, betas = np.zeros(x.shape[1]), theta = np.zeros(x.shape[1]),  step = stepInit, lam = lam, maxIter = 100)[-1]\n",
    "    \n",
    "        #Find predictions for our test set\n",
    "        pred = 1/(1+np.exp(-xTest.dot(TempBetas))) \n",
    "        Preds[:,i] =deepcopy(pred) #Capture predictions for each run through data\n",
    "    #print(Preds)\n",
    "    #Select prediction with highest probability and create the confusion matrix\n",
    "    MaxProbIndex = np.argmax(Preds, axis = 1)\n",
    "    for k in range(len(yTest)):\n",
    "        column = MaxProbIndex[k]\n",
    "        row = np.where(Cs == yTest[k])[0]\n",
    "        ConfMat[row, column] = ConfMat[row, column] + 1\n",
    "            #ConfMat[np.where(Cs == Preds[k])[0],np.where(Cs == YTest[k])[0]] = ConfMat[np.where(Cs == Preds[k])[0],np.where(Cs == YTest[k])[0]]+ 1\n",
    "    preds = Cs[MaxProbIndex]\n",
    "    MultiClassError = np.mean(preds != yTest)\n",
    "    return (ConfMat, preds, MultiClassError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "a\n",
      "a\n",
      "\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alyssa\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "The errors corresponding to lambdas are: [0.56597222222222221, 0.56597222222222221, 0.56597222222222221, 0.56597222222222221, 0.56712962962962965, 0.56481481481481477, 0.56134259259259256, 0.51851851851851849, 0.5625]\n"
     ]
    }
   ],
   "source": [
    "# Let's cross validate with a few values of lambda! Oh. Bummer the error rate is super high in all of them! \n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "pool = ThreadPool(7)\n",
    "lambdas = (.00001, .0001, .001, .01, .1, 1, 10, 100, 1000)\n",
    "#Corinne, thanks for tip on using print statements to test whether we are parallel processing despite global variables. We are. \n",
    "def OneVRestCrossVal(lam):\n",
    "    print(\"a\")\n",
    "    Matrix, Prediction, Error = confusionMatrixBig(xTrain = xTrain, yTrain = yTrain, xTest = xTest, yTest = yTest,  lam = lam)  \n",
    "    return(Error)\n",
    "resultsOvO = pool.map(OneVRestCrossVal, lambdas)\n",
    "pool.close\n",
    "pool.join\n",
    "       \n",
    "print(\"The errors corresponding to lambdas are:\", resultsOvO)\n",
    "print(\"The lambda corresponding to the lowest error is 100. However, the error is still quite high at nearly 52%!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the classification error for each category at each value in an array of lambdas. We can then choose the best lambda for each category\n",
    "def LambdaMatrix( xTrain, yTrain, xTest, yTest, lambdas, trainPCT = .75):\n",
    "    Cs = np.unique(y)\n",
    "    nt =int(len(y)/len(Cs) - round(len(y)/len(Cs) * trainPCT))*2 #length of test set \n",
    "    #initialize dictionaries to store the data sets \n",
    "    errorTrack = np.zeros((len(Cs), len(lambdas)))\n",
    "\n",
    "    myBetas={}\n",
    "    for i in range(len(Cs)):\n",
    "        c = Cs[i]\n",
    "        #ConfMat[0,i] = c #label axes of confusion matrix\n",
    "        #ConfMat[i,0] = c\n",
    " \n",
    "        #generate data\n",
    "        xTrainTemp, yTrainTemp,yTrainBinTemp = subset_dataAllC(x = xTrain, y = yTrain, c = c)\n",
    "        yTestBinTemp = (yTest == c) *2 -1\n",
    "        for j in range(len(lambdas)):\n",
    "            lam = lambdas[j]\n",
    "            #Calculate step size\n",
    "            stepInit = calcStepInit (x = xTrainTemp, y = yTrainTemp, lam = lam)\n",
    "       \n",
    "            #Train model\n",
    "            TempBetas = fastgradalgo(x=xTrainTemp, y = yTrainBinTemp, betas = np.zeros(x.shape[1]), theta = np.zeros(x.shape[1]),  step = stepInit, lam = lam, maxIter = 100)[-1]\n",
    "    \n",
    "        #Find the error rate using each set of model\n",
    "            pred = 1/(1+np.exp(-xTest.dot(TempBetas))) > .5\n",
    "            pred = pred*2- 1 # Convert to +/− 1 \n",
    "            error = np.mean(pred !=yTestBinTemp)\n",
    "            errorTrack[i, j] = error\n",
    "   \n",
    "    errorTrack = pd.DataFrame(data=errorTrack, index=Cats[Cs], columns=lambdas)\n",
    "    return errorTrack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0001</th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.1</th>\n",
       "      <th>1.0</th>\n",
       "      <th>10.0</th>\n",
       "      <th>100.0</th>\n",
       "      <th>1000.0</th>\n",
       "      <th>10000.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>White_Breasted_Kingfisher</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White_Breasted_Nuthatch</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White_Crowned_Sparrow</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White_Eyed_Vireo</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White_Necked_Raven</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0.0001      0.0010      0.0100      0.1000      \\\n",
       "White_Breasted_Kingfisher    0.166667    0.166667    0.166667    0.166667   \n",
       "White_Breasted_Nuthatch      0.166667    0.166667    0.166667    0.166667   \n",
       "White_Crowned_Sparrow        0.266667    0.266667    0.266667    0.266667   \n",
       "White_Eyed_Vireo             0.266667    0.266667    0.266667    0.266667   \n",
       "White_Necked_Raven           0.066667    0.066667    0.066667    0.066667   \n",
       "\n",
       "                           1.0000      10.0000     100.0000    1000.0000   \\\n",
       "White_Breasted_Kingfisher    0.166667    0.166667    0.166667    0.166667   \n",
       "White_Breasted_Nuthatch      0.166667    0.166667    0.166667    0.166667   \n",
       "White_Crowned_Sparrow        0.266667    0.266667    0.266667    0.266667   \n",
       "White_Eyed_Vireo             0.266667    0.266667    0.266667    0.266667   \n",
       "White_Necked_Raven           0.066667    0.066667    0.066667    0.066667   \n",
       "\n",
       "                           10000.0000  \n",
       "White_Breasted_Kingfisher    0.200000  \n",
       "White_Breasted_Nuthatch      0.166667  \n",
       "White_Crowned_Sparrow        0.266667  \n",
       "White_Eyed_Vireo             0.266667  \n",
       "White_Necked_Raven           0.066667  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lams = np.array((.0001,.001, .01, .1, 1, 10, 100, 1000,10000))\n",
    "\n",
    "LambdaMatrix( xTrain = xTrain, yTrain = yTrain, xTest = xTest, yTest = yTest, lambdas = lams, trainPCT = .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find the lambda value that minimizes the misclassification error of the test set for each category\n",
    "# lambdaMat = np.array(LambdaMatrix( xTrain = xTrain, yTrain = yTrain, xTest = xTest, yTest = yTest, lambdas = lams, trainPCT = .75))\n",
    "minErrors = np.argmin(lambdaMat, axis = 1)\n",
    "bestLams = lams[minErrors]\n",
    "np.savetxt(\"CrossValBestLamOnevRest.csv\", bestLams, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestLams = pd.read_csv('CrossValBestLamOnevRest.csv', sep=',', header=None)\n",
    "bestLams = np.array(crossValidatedLams)\n",
    "BestLamConfMat, bestLamMultiClassError, bestLamPreds = OptLamConfMat(xTest = xTest, yTest = yTest, xTrain = xTrain, yTrain = yTrain, Lams = bestLams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.670138888889\n"
     ]
    }
   ],
   "source": [
    "#Print the misclassification error using the one vs rest and my logistic regression function. \n",
    "#Sweet! My estimated misclassification error on the training set is in the ball park of scikit learn! \n",
    "print(bestLamMultiClassError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make predictions based on best lambdas. \n",
    "BestLamPredictions =  OptLamConfMat(xTest = testFeatures, yTest = testLabels, xTrain = x, yTrain = y, Lams = bestLams)[2]\n",
    "predstoFile(BestLamPredictions, 'myOnevRest.csv')\n",
    "# Too bad when I drop the predictions it creates in to Kaggle I get ~40% accuracy. Bummer!  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
